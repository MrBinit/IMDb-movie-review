{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mrbinit/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gensim\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 25000\n",
      "Test dataset length: 10000\n",
      "Validation dataset length: 15000\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(directory, label):\n",
    "    data = []\n",
    "    for filename in os.listdir(directory):\n",
    "        with open(os.path.join(directory, filename), 'r') as file:\n",
    "            review = file.read()\n",
    "            data.append((review, label))\n",
    "    return data\n",
    "\n",
    "directory_path = \"/Users/mrbinit/Downloads/aclImdb\" \n",
    "\n",
    "train_pos_dir = os.path.join(directory_path, 'train', 'pos')\n",
    "train_neg_dir = os.path.join(directory_path, 'train', 'neg')\n",
    "test_pos_dir = os.path.join(directory_path, 'test', 'pos')\n",
    "test_neg_dir = os.path.join(directory_path, 'test', 'neg')\n",
    "val_pos_dir = os.path.join(directory_path, 'val', 'pos')\n",
    "val_neg_dir = os.path.join(directory_path, 'val', 'neg')\n",
    "\n",
    "train_data = load_dataset(train_pos_dir, 1) + load_dataset(train_neg_dir, 0) #1 represents positive and 0 represents negative sentiments\n",
    "test_data = load_dataset(test_pos_dir, 1) + load_dataset(test_neg_dir, 0)\n",
    "\n",
    "#split the test set into a validation set (15,000 samples) and a test set (10,000 samples)\n",
    "val_data = test_data[:15000]\n",
    "test_data = test_data[15000:25000]\n",
    "\n",
    "#separate the reviews and labels from the train, test, and validation data\n",
    "train_reviews, train_labels = zip(*train_data)\n",
    "test_reviews, test_labels = zip(*test_data)\n",
    "val_reviews, val_labels = zip(*val_data)\n",
    "\n",
    "#check the lengths of train, test, and validation datasets\n",
    "train_length = len(train_reviews)\n",
    "test_length = len(test_reviews)\n",
    "val_length = len(val_reviews)\n",
    "\n",
    "print(\"Train dataset length:\", train_length)\n",
    "print(\"Test dataset length:\", test_length)\n",
    "print(\"Validation dataset length:\", val_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data: ('For a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. Imagine a movie where Joe Piscopo is actually funny! Maureen Stapleton is a scene stealer. The Moroni character is an absolute scream. Watch for Alan \"The Skipper\" Hale jr. as a police Sgt.', 'Bizarre horror movie filled with famous faces but stolen by Cristina Raines (later of TV\\'s \"Flamingo Road\") as a pretty but somewhat unstable model with a gummy smile who is slated to pay for her attempted suicides by guarding the Gateway to Hell! The scenes with Raines modeling are very well captured, the mood music is perfect, Deborah Raffin is charming as Cristina\\'s pal, but when Raines moves into a creepy Brooklyn Heights brownstone (inhabited by a blind priest on the top floor), things really start cooking. The neighbors, including a fantastically wicked Burgess Meredith and kinky couple Sylvia Miles & Beverly D\\'Angelo, are a diabolical lot, and Eli Wallach is great fun as a wily police detective. The movie is nearly a cross-pollination of \"Rosemary\\'s Baby\" and \"The Exorcist\"--but what a combination! Based on the best-seller by Jeffrey Konvitz, \"The Sentinel\" is entertainingly spooky, full of shocks brought off well by director Michael Winner, who mounts a thoughtfully downbeat ending with skill. ***1/2 from ****', 'A solid, if unremarkable film. Matthau, as Einstein, was wonderful. My favorite part, and the only thing that would make me go out of my way to see this again, was the wonderful scene with the physicists playing badmitton, I loved the sweaters and the conversation while they waited for Robbins to retrieve the birdie.', 'It\\'s a strange feeling to sit alone in a theater occupied by parents and their rollicking kids. I felt like instead of a movie ticket, I should have been given a NAMBLA membership.<br /><br />Based upon Thomas Rockwell\\'s respected Book, How To Eat Fried Worms starts like any children\\'s story: moving to a new town. The new kid, fifth grader Billy Forrester was once popular, but has to start anew. Making friends is never easy, especially when the only prospect is Poindexter Adam. Or Erica, who at 4 1/2 feet, is a giant.<br /><br />Further complicating things is Joe the bully. His freckled face and sleeveless shirts are daunting. He antagonizes kids with the Death Ring: a Crackerjack ring that is rumored to kill you if you\\'re punched with it. But not immediately. No, the death ring unleashes a poison that kills you in the eight grade.<br /><br />Joe and his axis of evil welcome Billy by smuggling a handful of slimy worms into his thermos. Once discovered, Billy plays it cool, swearing that he eats worms all the time. Then he throws them at Joe\\'s face. Ewww! To win them over, Billy reluctantly bets that he can eat 10 worms. Fried, boiled, marinated in hot sauce, squashed and spread on a peanut butter sandwich. Each meal is dubbed an exotic name like the \"Radioactive Slime Delight,\" in which the kids finally live out their dream of microwaving a living organism.<br /><br />If you\\'ve ever met me, you\\'ll know that I have an uncontrollably hearty laugh. I felt like a creep erupting at a toddler whining that his \"dilly dick\" hurts. But Fried Worms is wonderfully disgusting. Like a G-rated Farrelly brothers film, it is both vomitous and delightful.<br /><br />Writer/director Bob Dolman is also a savvy storyteller. To raise the stakes the worms must be consumed by 7 pm. In addition Billy holds a dark secret: he has an ultra-sensitive stomach.<br /><br />Dolman also has a keen sense of perspective. With such accuracy, he draws on children\\'s insecurities and tendency to exaggerate mundane dilemmas.<br /><br />If you were to hyperbolize this movie the way kids do their quandaries, you will see that it is essentially about war. Freedom-fighter and freedom-hater use pubescent boys as pawns in proxy wars, only to learn a valuable lesson in unity. International leaders can learn a thing or two about global peacekeeping from Fried Worms.<br /><br />At the end of the film, I was comforted when two chaperoning mothers behind me, looked at each other with befuddlement and agreed, \"That was a great movie.\" Great, now I won\\'t have to register myself in any lawful databases.', \"You probably all already know this by now, but 5 additional episodes never aired can be viewed on ABC.com I've watched a lot of television over the years and this is possibly my favorite show, ever. It's a crime that this beautifully written and acted show was canceled. The actors that played Laura, Whit, Carlos, Mae, Damian, Anya and omg, Steven Caseman - are all incredible and so natural in those roles. Even the kids are great. Wonderful show. So sad that it's gone. Of course I wonder about the reasons it was canceled. There is no way I'll let myself believe that Ms. Moynahan's pregnancy had anything to do with it. It was in the perfect time slot in this market. I've watched all the episodes again on ABC.com - I hope they all come out on DVD some day. Thanks for reading.\")\n",
      "Test Data: ('Oh how I laughed....this has it all...an Asian/White family, a disabled Asian boy...everything a healthy person needs to see in the eyes of the BBC.<br /><br />What utter tribe: This was a total insult to my eyes that viewed this rubbish for one episode and ONE EPISODE ONLY.<br /><br />When you think of some of the quality the BBC has put out over the years (Fawlty Towers for example) and then this comes rolling in...Its a disgusting disgrace.<br /><br />Its all geared on political-correctness and is devoid of any humour whatsoever.<br /><br />This is straight from the bowels of hell: but what would you expect from the ultra left-wing BPC...I mean BBC.', 'I saw this movie on video with a couple of friends as part of a teen comedy triple header, alongside Dorm Daze and Going Greek. Obviously we weren\\'t going for quality, but for air-headed entertainment and gross-out gags. Generally we got what we came for, but Wild Roomies really stood out: For being awful.<br /><br />First impression first, the cover: The Norwegian release cover showed breasts covered up and a bunch of seemingly \"crazy\" roomies in the background; Brings up pictures of drunken semi-naked teenagers doing bunches of funny stuff stuff doesn\\'t it?- The \"crazy\" roomies on the cover were not even featured in the picture. And the funny stuff you assume they\\'ll be doing?- Absent as well.<br /><br />This movie was labeled comedy, which is a bit strange since it not even remotely funny. Relationship drama would be more accurate.<br /><br />Put shortly the movie is about a young couple inheriting a swanky house in LA, and are forced to get some roomies to make ends meet. These roommates eventually put both the young couple\\'s patience and relationship to the test because they\\'re so \"wild\". Problem is, they\\'re not. And here lies the problem of this movie: The protagonist couple are so anal-retentive and neurotic that they manage to generate zero sympathy. You\\'ll find yourself rooting for the antagonist roomies halfway through the movie. Add mediocre acting, lame dialogue and boring direction and you got yourself a movie that is best left unseen.<br /><br />See something else.', 'This movie should go down in the annals of fiefdom as one of the worst of all time. I will stop short of saying it\\'s the worst movie ever, only because I have yet to see every movie ever made. I cannot make such lofty claims until then. The story is stale, the acting is horrible, at best, the \"special\" effects are no more than a couple of lbs. of dry ice and a fan. Somebody must have been related to someone to get this movie made. Mr. Busey mailed this one in! The dog is well trained and cute, making it the only redeeming quality in this never-should-have-made-it movie. Two hours and $3 of my life I will never get back.', 'This show is the worst show ever! Norris and his family write it, produce it, direct it, etc etc. The only reason I ever see it is because my goofy wife likes it. How many times can Norris fly though the air from plain sight to land a kick on an obviously blind villain? No trees, no building, just whoosh.....thin air. He ALWAYS solves the case or is the best at whatever skill there is. No co star ever gets the glory. Its all Norris. Its truly apparent that Norris is awful stuck on himself and will not allow anyone to one up him in any scene no matter what the content. Terrible acting, terrible script, terrible series.', 'It looks like people involved with this movie are stuffing the ballot box to boost its ratings. The good news that apparently only 18 people have seen it. I suppose that makes me the 19th. I have no involvement with the flick and don\\'t know anyone who did and I\\'m a long-time IMDb user (check my vote record and reviews over the past seven years), so I promise I\\'m giving an honest and unbiased opinion. It\\'s coming to you from a 30-year horror fan who has also appeared in a couple of low-budget flicks himself.<br /><br />Aside from a couple of interesting video effects, \"Frankensteins Bloody Nightmare\" is incoherent, boring, and technically flawed beyond all reason. It was apparently shot on silent stock and the audio then dubbed in; most of it sounds like it was recorded with a tin can and a piece of string, anyhow. More than three quarters of the dialog is inaudible.<br /><br />I watched this from beginning to end and have no idea of what the story was, or even if there was one. It seems like the director is mostly impressing himself with long, panning shots of the corners of table and dead black spaces that do nothing but pad the film out. That would be a problem if one were actually developing a plot and making a film that had some sense of pacing. In this case, though, the rule doesn\\'t apply. It doesn\\'t matter how scenes are shot because they don\\'t add up to a story.<br /><br />Watching this video is an exercise in futility at every level. Whatever people who worked on it are writing and however they\\'re trying to influence the ratings here on IMDb, this is just bad, tedious stuff.<br /><br />That\\'s the honest truth. If you\\'re thinking of spending your money or time on this one, think again. It\\'s easy to find something better because you won\\'t find much worse.<br /><br />And that\\'s the unbiased, unvarnished truth.')\n",
      "Validation Data: ('Based on an actual story, John Boorman shows the struggle of an American doctor, whose husband and son were murdered and she was continually plagued with her loss. A holiday to Burma with her sister seemed like a good idea to get away from it all, but when her passport was stolen in Rangoon, she could not leave the country with her sister, and was forced to stay back until she could get I.D. papers from the American embassy. To fill in a day before she could fly out, she took a trip into the countryside with a tour guide. \"I tried finding something in those stone statues, but nothing stirred in me. I was stone myself.\" <br /><br />Suddenly all hell broke loose and she was caught in a political revolt. Just when it looked like she had escaped and safely boarded a train, she saw her tour guide get beaten and shot. In a split second she decided to jump from the moving train and try to rescue him, with no thought of herself. Continually her life was in danger. <br /><br />Here is a woman who demonstrated spontaneous, selfless charity, risking her life to save another. Patricia Arquette is beautiful, and not just to look at; she has a beautiful heart. This is an unforgettable story. <br /><br />\"We are taught that suffering is the one promise that life always keeps.\"', \"This is a gem. As a Film Four production - the anticipated quality was indeed delivered. Shot with great style that reminded me some Errol Morris films, well arranged and simply gripping. It's long yet horrifying to the point it's excruciating. We know something bad happened (one can guess by the lack of participation of a person in the interviews) but we are compelled to see it, a bit like a car accident in slow motion. The story spans most conceivable aspects and unlike some documentaries did not try and refrain from showing the grimmer sides of the stories, as also dealing with the guilt of the people Don left behind him, wondering why they didn't stop him in time. It took me a few hours to get out of the melancholy that gripped me after seeing this very-well made documentary.\", \"I really like this show. It has drama, romance, and comedy all rolled into one. I am 28 and I am a married mother, so I can identify both with Lorelei's and Rory's experiences in the show. I have been watching mostly the repeats on the Family Channel lately, so I am not up-to-date on what is going on now. I think females would like this show more than males, but I know some men out there would enjoy it! I really like that is an hour long and not a half hour, as th hour seems to fly by when I am watching it! Give it a chance if you have never seen the show! I think Lorelei and Luke are my favorite characters on the show though, mainly because of the way they are with one another. How could you not see something was there (or take that long to see it I guess I should say)? <br /><br />Happy viewing!\", 'This is the best 3-D experience Disney has at their themeparks. This is certainly better than their original 1960\\'s acid-trip film that was in it\\'s place, is leagues better than \"Honey I Shrunk The Audience\" (and far more fun), barely squeaks by the MuppetVision 3-D movie at Disney-MGM and can even beat the original 3-D \"Movie Experience\" Captain EO. This film relives some of Disney\\'s greatest musical hits from Aladdin, The Little Mermaid, and others, and brought a smile to my face throughout the entire show. This is a totally kid-friendly movie too, unlike \"Honey...\" and has more effects than the spectacular \"MuppetVision\"', \"Of the Korean movies I've seen, only three had really stuck with me. The first is the excellent horror A Tale of Two Sisters. The second and third - and now fourth too - have all been Park Chan Wook's movies, namely Oldboy, Sympathy for Lady Vengeance), and now Thirst. <br /><br />Park kinda reminds me of Quentin Tarantino with his irreverence towards convention. All his movies are shocking, but not in a gratuitous sense. It's more like he shows us what we don't expect to see - typically situations that go radically against society's morals, like incest or a libidinous, blood-sucking, yet devout priest. He's also quite artistically-inclined with regards to cinematography, and his movies are among the more gorgeous that I've seen.<br /><br />Thirst is all that - being about said priest and the repressed, conscience-less woman he falls for - and more. It's horror, drama, and even comedy, as Park disarms his audience with many inappropriate yet humorous situations. As such, this might be his best work for me yet, since his other two movies that I've seen were lacking the humor element that would've made them more palatable for repeat viewings.\")\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Data:\", train_reviews[:5])  \n",
    "print(\"Test Data:\", test_reviews[:5])   \n",
    "print(\"Validation Data:\", val_reviews[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset contains HTML tags: True\n",
      "Test dataset contains HTML tags: True\n",
      "Validation dataset contains HTML tags: True\n"
     ]
    }
   ],
   "source": [
    "#Regular expressions (regex) are sequences of characters that define a search pattern. They are used for string manipulation, searching, and pattern matching within text. \n",
    "import re\n",
    "def has_html_tags(text):\n",
    "    pattern = re.compile(r'<[^>]+>')  # Regular expression to match HTML tags\n",
    "    return bool(pattern.search(text))\n",
    "\n",
    "# Check for HTML tags in each dataset\n",
    "def check_html_tags(dataset):\n",
    "    for review, _ in dataset:\n",
    "        if has_html_tags(review):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "#check for HTML tags in each dataset\n",
    "train_has_html = check_html_tags(train_data)\n",
    "test_has_html = check_html_tags(test_data)\n",
    "val_has_html = check_html_tags(val_data)\n",
    "#print output\n",
    "print(\"Train dataset contains HTML tags:\", train_has_html)\n",
    "print(\"Test dataset contains HTML tags:\", test_has_html)\n",
    "print(\"Validation dataset contains HTML tags:\", val_has_html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset contains URLs: True\n",
      "Test dataset contains URLs: True\n",
      "Validation dataset contains URLs: True\n"
     ]
    }
   ],
   "source": [
    "def has_url(text):\n",
    "    pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    return bool(pattern.search(text))\n",
    "\n",
    "#condition to check for url\n",
    "def check_for_urls(dataset):\n",
    "    for review, _ in dataset:\n",
    "        if has_url(review):\n",
    "            return True\n",
    "    return False\n",
    "#check whether there is URL or not\n",
    "train_has_url = check_for_urls(train_data)\n",
    "test_has_url = check_for_urls(test_data)\n",
    "val_has_url = check_for_urls(val_data)\n",
    "\n",
    "print(\"Train dataset contains URLs:\", train_has_url)\n",
    "print(\"Test dataset contains URLs:\", test_has_url)\n",
    "print(\"Validation dataset contains URLs:\", val_has_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset contains special characters: True\n",
      "Test dataset contains special characters: True\n",
      "Validation dataset contains special characters: True\n"
     ]
    }
   ],
   "source": [
    "def has_special_characters(text):\n",
    "    pattern = re.compile(r'[^a-zA-Z0-9\\s]')\n",
    "    return bool(pattern.search(text))\n",
    "\n",
    "def check_for_special_characters(dataset):\n",
    "    for review, _ in dataset:\n",
    "        if has_special_characters(review):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "train_has_special_chars = check_for_special_characters(train_data)\n",
    "test_has_special_chars = check_for_special_characters(test_data)\n",
    "val_has_special_chars = check_for_special_characters(val_data)\n",
    "print(\"Train dataset contains special characters:\", train_has_special_chars)\n",
    "print(\"Test dataset contains special characters:\", test_has_special_chars)\n",
    "print(\"Validation dataset contains special characters:\", val_has_special_chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess function\n",
    "def preprocess_text(text):\n",
    "    #remove  HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "    #remove urls\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "    #expand contractions \n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    #tokenize \n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    #remove non-alphabetic tokens and convert to lowercase\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "\n",
    "    #remove stopwords \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if not word in stop_words]\n",
    "\n",
    "    #lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_7/q65t_5bs3653ct73dh3s10w00000gn/T/ipykernel_92017/3997674309.py:4: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n"
     ]
    }
   ],
   "source": [
    "# Preprocess all the data\n",
    "train_reviews_processed = [preprocess_text(review) for review in train_reviews]\n",
    "test_reviews_processed = [preprocess_text(review) for review in test_reviews]\n",
    "val_reviews_processed = [preprocess_text(review) for review in val_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Train Data: [['movie', 'get', 'respect', 'sure', 'lot', 'memorable', 'quote', 'listed', 'gem', 'imagine', 'movie', 'joe', 'piscopo', 'actually', 'funny', 'maureen', 'stapleton', 'scene', 'stealer', 'moroni', 'character', 'absolute', 'scream', 'watch', 'alan', 'skipper', 'hale', 'police', 'sgt'], ['bizarre', 'horror', 'movie', 'filled', 'famous', 'face', 'stolen', 'cristina', 'raines', 'later', 'tv', 'flamingo', 'road', 'pretty', 'somewhat', 'unstable', 'model', 'gummy', 'smile', 'slated', 'pay', 'attempted', 'suicide', 'guarding', 'gateway', 'hell', 'scene', 'raines', 'modeling', 'well', 'captured', 'mood', 'music', 'perfect', 'deborah', 'raffin', 'charming', 'cristina', 'pal', 'raines', 'move', 'creepy', 'brooklyn', 'height', 'brownstone', 'inhabited', 'blind', 'priest', 'top', 'floor', 'thing', 'really', 'start', 'cooking', 'neighbor', 'including', 'fantastically', 'wicked', 'burgess', 'meredith', 'kinky', 'couple', 'sylvia', 'mile', 'beverly', 'diabolical', 'lot', 'eli', 'wallach', 'great', 'fun', 'wily', 'police', 'detective', 'movie', 'nearly', 'rosemary', 'baby', 'exorcist', 'combination', 'based', 'jeffrey', 'konvitz', 'sentinel', 'entertainingly', 'spooky', 'full', 'shock', 'brought', 'well', 'director', 'michael', 'winner', 'mount', 'thoughtfully', 'downbeat', 'ending', 'skill'], ['solid', 'unremarkable', 'film', 'matthau', 'einstein', 'wonderful', 'favorite', 'part', 'thing', 'would', 'make', 'go', 'way', 'see', 'wonderful', 'scene', 'physicist', 'playing', 'badmitton', 'loved', 'sweater', 'conversation', 'waited', 'robbins', 'retrieve', 'birdie'], ['strange', 'feeling', 'sit', 'alone', 'theater', 'occupied', 'parent', 'rollicking', 'kid', 'felt', 'like', 'instead', 'movie', 'ticket', 'given', 'nambla', 'upon', 'thomas', 'rockwell', 'respected', 'book', 'eat', 'fried', 'worm', 'start', 'like', 'child', 'story', 'moving', 'new', 'town', 'new', 'kid', 'fifth', 'grader', 'billy', 'forrester', 'popular', 'start', 'anew', 'making', 'friend', 'never', 'easy', 'especially', 'prospect', 'poindexter', 'adam', 'erica', 'foot', 'complicating', 'thing', 'joe', 'bully', 'freckled', 'face', 'sleeveless', 'shirt', 'daunting', 'antagonizes', 'kid', 'death', 'ring', 'crackerjack', 'ring', 'rumored', 'kill', 'punched', 'immediately', 'death', 'ring', 'unleashes', 'poison', 'kill', 'eight', 'axis', 'evil', 'welcome', 'billy', 'smuggling', 'handful', 'slimy', 'worm', 'thermos', 'discovered', 'billy', 'play', 'cool', 'swearing', 'eats', 'worm', 'time', 'throw', 'joe', 'face', 'ewww', 'win', 'billy', 'reluctantly', 'bet', 'eat', 'worm', 'fried', 'boiled', 'marinated', 'hot', 'sauce', 'squashed', 'spread', 'peanut', 'butter', 'sandwich', 'meal', 'dubbed', 'exotic', 'name', 'like', 'radioactive', 'slime', 'delight', 'kid', 'finally', 'live', 'dream', 'microwaving', 'living', 'ever', 'met', 'know', 'uncontrollably', 'hearty', 'laugh', 'felt', 'like', 'creep', 'erupting', 'toddler', 'whining', 'dilly', 'dick', 'hurt', 'fried', 'worm', 'wonderfully', 'disgusting', 'like', 'farrelly', 'brother', 'film', 'vomitous', 'bob', 'dolman', 'also', 'savvy', 'storyteller', 'raise', 'stake', 'worm', 'must', 'consumed', 'pm', 'addition', 'billy', 'hold', 'dark', 'secret', 'also', 'keen', 'sense', 'perspective', 'accuracy', 'draw', 'child', 'insecurity', 'tendency', 'exaggerate', 'mundane', 'hyperbolize', 'movie', 'way', 'kid', 'quandary', 'see', 'essentially', 'war', 'use', 'pubescent', 'boy', 'pawn', 'proxy', 'war', 'learn', 'valuable', 'lesson', 'unity', 'international', 'leader', 'learn', 'thing', 'two', 'global', 'peacekeeping', 'fried', 'end', 'film', 'comforted', 'two', 'chaperoning', 'mother', 'behind', 'looked', 'befuddlement', 'agreed', 'great', 'movie', 'great', 'register', 'lawful', 'database'], ['probably', 'already', 'know', 'additional', 'episode', 'never', 'aired', 'viewed', 'watched', 'lot', 'television', 'year', 'possibly', 'favorite', 'show', 'ever', 'crime', 'beautifully', 'written', 'acted', 'show', 'canceled', 'actor', 'played', 'laura', 'whit', 'carlos', 'mae', 'damian', 'anya', 'omg', 'steven', 'caseman', 'incredible', 'natural', 'role', 'even', 'kid', 'great', 'wonderful', 'show', 'sad', 'gone', 'course', 'wonder', 'reason', 'canceled', 'way', 'let', 'believe', 'moynahan', 'pregnancy', 'anything', 'perfect', 'time', 'slot', 'market', 'watched', 'episode', 'hope', 'come', 'dvd', 'day', 'thanks', 'reading']]\n",
      "Cleaned Test Data: [['oh', 'laughed', 'family', 'disabled', 'asian', 'boy', 'everything', 'healthy', 'person', 'need', 'see', 'eye', 'utter', 'tribe', 'total', 'insult', 'eye', 'viewed', 'rubbish', 'one', 'episode', 'one', 'episode', 'think', 'quality', 'bbc', 'put', 'year', 'fawlty', 'tower', 'example', 'come', 'rolling', 'disgusting', 'geared', 'devoid', 'humour', 'straight', 'bowel', 'hell', 'would', 'expect', 'ultra', 'bpc', 'mean', 'bbc'], ['saw', 'movie', 'video', 'couple', 'friend', 'part', 'teen', 'comedy', 'triple', 'header', 'alongside', 'dorm', 'daze', 'going', 'greek', 'obviously', 'going', 'quality', 'entertainment', 'gag', 'generally', 'got', 'came', 'wild', 'roomy', 'really', 'stood', 'impression', 'first', 'cover', 'norwegian', 'release', 'cover', 'showed', 'breast', 'covered', 'bunch', 'seemingly', 'crazy', 'roomy', 'background', 'brings', 'picture', 'drunken', 'teenager', 'bunch', 'funny', 'stuff', 'stuff', 'crazy', 'roomy', 'cover', 'even', 'featured', 'picture', 'funny', 'stuff', 'assume', 'absent', 'movie', 'labeled', 'comedy', 'bit', 'strange', 'since', 'even', 'remotely', 'funny', 'relationship', 'drama', 'would', 'shortly', 'movie', 'young', 'couple', 'inheriting', 'swanky', 'house', 'la', 'forced', 'get', 'roomy', 'make', 'end', 'meet', 'roommate', 'eventually', 'put', 'young', 'couple', 'patience', 'relationship', 'test', 'wild', 'problem', 'lie', 'problem', 'movie', 'protagonist', 'couple', 'neurotic', 'manage', 'generate', 'zero', 'sympathy', 'find', 'rooting', 'antagonist', 'roomy', 'halfway', 'movie', 'add', 'mediocre', 'acting', 'lame', 'dialogue', 'boring', 'direction', 'got', 'movie', 'best', 'left', 'something', 'else'], ['movie', 'go', 'annals', 'fiefdom', 'one', 'worst', 'time', 'stop', 'short', 'saying', 'worst', 'movie', 'ever', 'yet', 'see', 'every', 'movie', 'ever', 'made', 'make', 'lofty', 'claim', 'story', 'stale', 'acting', 'horrible', 'best', 'special', 'effect', 'couple', 'lb', 'dry', 'ice', 'fan', 'somebody', 'must', 'related', 'someone', 'get', 'movie', 'made', 'busey', 'mailed', 'one', 'dog', 'well', 'trained', 'cute', 'making', 'redeeming', 'quality', 'movie', 'two', 'hour', 'life', 'never', 'get', 'back'], ['show', 'worst', 'show', 'ever', 'norris', 'family', 'write', 'produce', 'direct', 'etc', 'etc', 'reason', 'ever', 'see', 'goofy', 'wife', 'like', 'many', 'time', 'norris', 'fly', 'though', 'air', 'plain', 'sight', 'land', 'kick', 'obviously', 'blind', 'villain', 'tree', 'building', 'whoosh', 'thin', 'air', 'always', 'solves', 'case', 'best', 'whatever', 'skill', 'co', 'star', 'ever', 'get', 'glory', 'norris', 'truly', 'apparent', 'norris', 'awful', 'stuck', 'allow', 'anyone', 'one', 'scene', 'matter', 'content', 'terrible', 'acting', 'terrible', 'script', 'terrible', 'series'], ['look', 'like', 'people', 'involved', 'movie', 'stuffing', 'ballot', 'box', 'boost', 'rating', 'good', 'news', 'apparently', 'people', 'seen', 'suppose', 'make', 'involvement', 'flick', 'know', 'anyone', 'imdb', 'user', 'check', 'vote', 'record', 'review', 'past', 'seven', 'year', 'promise', 'giving', 'honest', 'unbiased', 'opinion', 'coming', 'horror', 'fan', 'also', 'appeared', 'couple', 'flick', 'couple', 'interesting', 'video', 'effect', 'frankenstein', 'bloody', 'nightmare', 'incoherent', 'boring', 'technically', 'flawed', 'beyond', 'reason', 'apparently', 'shot', 'silent', 'stock', 'audio', 'dubbed', 'sound', 'like', 'recorded', 'tin', 'piece', 'string', 'anyhow', 'three', 'quarter', 'dialog', 'watched', 'beginning', 'end', 'idea', 'story', 'even', 'one', 'seems', 'like', 'director', 'mostly', 'impressing', 'long', 'panning', 'shot', 'corner', 'table', 'dead', 'black', 'space', 'nothing', 'pad', 'film', 'would', 'problem', 'one', 'actually', 'developing', 'plot', 'making', 'film', 'sense', 'pacing', 'case', 'though', 'rule', 'apply', 'matter', 'scene', 'shot', 'add', 'video', 'exercise', 'futility', 'every', 'level', 'whatever', 'people', 'worked', 'writing', 'however', 'trying', 'influence', 'rating', 'imdb', 'bad', 'tedious', 'honest', 'truth', 'thinking', 'spending', 'money', 'time', 'one', 'think', 'easy', 'find', 'something', 'better', 'find', 'much', 'unbiased', 'unvarnished', 'truth']]\n",
      "Cleaned Validation Data: [['based', 'actual', 'story', 'john', 'boorman', 'show', 'struggle', 'american', 'doctor', 'whose', 'husband', 'son', 'murdered', 'continually', 'plagued', 'loss', 'holiday', 'burma', 'sister', 'seemed', 'like', 'good', 'idea', 'get', 'away', 'passport', 'stolen', 'rangoon', 'could', 'leave', 'country', 'sister', 'forced', 'stay', 'back', 'could', 'get', 'paper', 'american', 'embassy', 'fill', 'day', 'could', 'fly', 'took', 'trip', 'countryside', 'tour', 'guide', 'tried', 'finding', 'something', 'stone', 'statue', 'nothing', 'stirred', 'stone', 'suddenly', 'hell', 'broke', 'loose', 'caught', 'political', 'revolt', 'looked', 'like', 'escaped', 'safely', 'boarded', 'train', 'saw', 'tour', 'guide', 'get', 'beaten', 'shot', 'split', 'second', 'decided', 'jump', 'moving', 'train', 'try', 'rescue', 'thought', 'continually', 'life', 'danger', 'woman', 'demonstrated', 'spontaneous', 'selfless', 'charity', 'risking', 'life', 'save', 'another', 'patricia', 'arquette', 'beautiful', 'look', 'beautiful', 'heart', 'unforgettable', 'story', 'taught', 'suffering', 'one', 'promise', 'life', 'always', 'keep'], ['gem', 'film', 'four', 'production', 'anticipated', 'quality', 'indeed', 'delivered', 'shot', 'great', 'style', 'reminded', 'errol', 'morris', 'film', 'well', 'arranged', 'simply', 'gripping', 'long', 'yet', 'horrifying', 'point', 'excruciating', 'know', 'something', 'bad', 'happened', 'one', 'guess', 'lack', 'participation', 'person', 'interview', 'compelled', 'see', 'bit', 'like', 'car', 'accident', 'slow', 'motion', 'story', 'span', 'conceivable', 'aspect', 'unlike', 'documentary', 'try', 'refrain', 'showing', 'grimmer', 'side', 'story', 'also', 'dealing', 'guilt', 'people', 'left', 'behind', 'wondering', 'stop', 'time', 'took', 'hour', 'get', 'melancholy', 'gripped', 'seeing', 'made', 'documentary'], ['really', 'like', 'show', 'drama', 'romance', 'comedy', 'rolled', 'one', 'married', 'mother', 'identify', 'lorelei', 'rory', 'experience', 'show', 'watching', 'mostly', 'repeat', 'family', 'channel', 'lately', 'going', 'think', 'female', 'would', 'like', 'show', 'male', 'know', 'men', 'would', 'enjoy', 'really', 'like', 'hour', 'long', 'half', 'hour', 'th', 'hour', 'seems', 'fly', 'watching', 'give', 'chance', 'never', 'seen', 'show', 'think', 'lorelei', 'luke', 'favorite', 'character', 'show', 'though', 'mainly', 'way', 'one', 'another', 'could', 'see', 'something', 'take', 'long', 'see', 'guess', 'say', 'happy', 'viewing'], ['best', 'experience', 'disney', 'themeparks', 'certainly', 'better', 'original', 'film', 'place', 'league', 'better', 'honey', 'shrunk', 'audience', 'far', 'fun', 'barely', 'squeak', 'muppetvision', 'movie', 'even', 'beat', 'original', 'movie', 'experience', 'captain', 'eo', 'film', 'relives', 'disney', 'greatest', 'musical', 'hit', 'aladdin', 'little', 'mermaid', 'others', 'brought', 'smile', 'face', 'throughout', 'entire', 'show', 'totally', 'movie', 'unlike', 'honey', 'effect', 'spectacular', 'muppetvision'], ['korean', 'movie', 'seen', 'three', 'really', 'stuck', 'first', 'excellent', 'horror', 'tale', 'two', 'sister', 'second', 'third', 'fourth', 'park', 'chan', 'wook', 'movie', 'namely', 'oldboy', 'sympathy', 'lady', 'vengeance', 'thirst', 'park', 'kind', 'reminds', 'quentin', 'tarantino', 'irreverence', 'towards', 'convention', 'movie', 'shocking', 'gratuitous', 'sense', 'like', 'show', 'u', 'expect', 'see', 'typically', 'situation', 'go', 'radically', 'society', 'moral', 'like', 'incest', 'libidinous', 'yet', 'devout', 'priest', 'also', 'quite', 'regard', 'cinematography', 'movie', 'among', 'gorgeous', 'said', 'priest', 'repressed', 'woman', 'fall', 'horror', 'drama', 'even', 'comedy', 'park', 'disarms', 'audience', 'many', 'inappropriate', 'yet', 'humorous', 'situation', 'might', 'best', 'work', 'yet', 'since', 'two', 'movie', 'seen', 'lacking', 'humor', 'element', 'would', 'made', 'palatable', 'repeat', 'viewing']]\n"
     ]
    }
   ],
   "source": [
    "print(\"Cleaned Train Data:\", train_reviews_processed[:5])  \n",
    "print(\"Cleaned Test Data:\", test_reviews_processed[:5])   \n",
    "print(\"Cleaned Validation Data:\", val_reviews_processed[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 1394\n"
     ]
    }
   ],
   "source": [
    "def find_max_sequence_length(train_reviews_processed):\n",
    "    max_length = 0\n",
    "    for tokens in train_reviews_processed:\n",
    "        sequence_length = len(tokens)\n",
    "        if sequence_length > max_length:\n",
    "            max_length = sequence_length\n",
    "    return max_length\n",
    "\n",
    "#assuming tokenized_texts is a list of tokenized texts after preprocessing\n",
    "max_sequence_length = find_max_sequence_length(train_reviews_processed)\n",
    "print(\"Maximum sequence length:\", max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec model\n",
    "word2vec_model = gensim.models.Word2Vec(sentences=train_reviews_processed + test_reviews_processed + val_reviews_processed,\n",
    "                                        vector_size=100, window=5, min_count=5, workers=4)\n",
    "\n",
    "# Function to get vector representation of a sentence\n",
    "def get_sentence_vector(tokens):\n",
    "    vector = np.zeros((100,))\n",
    "    count = 0\n",
    "    for word in tokens:\n",
    "        if word in word2vec_model.wv:\n",
    "            vector += word2vec_model.wv[word]\n",
    "            count += 1\n",
    "    if count != 0:\n",
    "        vector /= count\n",
    "    return vector\n",
    "    \n",
    "#get sentence vector for all the dataset \n",
    "train_vectors = [get_sentence_vector(tokens) for tokens in train_reviews_processed]\n",
    "test_vectors = [get_sentence_vector(tokens) for tokens in test_reviews_processed]\n",
    "val_vectors = [get_sentence_vector(tokens) for tokens in val_reviews_processed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert lists to numpy arrays \n",
    "X_train = np.array(train_vectors)\n",
    "X_test = np.array(test_vectors)\n",
    "X_val = np.array(val_vectors)\n",
    "\n",
    "\n",
    "y_train = np.array(train_labels)\n",
    "y_test = np.array(test_labels)\n",
    "y_val = np.array(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m36,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m576\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m36,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">93,322</span> (364.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m93,322\u001b[0m (364.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">93,322</span> (364.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m93,322\u001b[0m (364.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define the CNN model\n",
    "def build_cnn_model(input_shape):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # Convolutional layers\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    \n",
    "    # Flatten layer\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    # Dense layers\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))  # Output layer with softmax activation for classification\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Input shape of your data (assuming you're dealing with image data)\n",
    "input_shape = (28, 28, 1)  # Example input shape (28x28 grayscale images)\n",
    "model = build_cnn_model(input_shape)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',  # Since the labels are integers\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Define CNN model\u001b[39;00m\n\u001b[1;32m     22\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[0;32m---> 23\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Embedding(input_dim\u001b[38;5;241m=\u001b[39m\u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocabulary_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, output_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, input_length\u001b[38;5;241m=\u001b[39mmax_len))\n\u001b[1;32m     24\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Conv1D(\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m5\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     25\u001b[0m model\u001b[38;5;241m.\u001b[39madd(MaxPooling1D(\u001b[38;5;241m5\u001b[39m))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "\n",
    "# Assuming train_reviews_processed, test_reviews_processed, and val_reviews_processed are defined\n",
    "# and contain lists of tokenized reviews.\n",
    "\n",
    "# Vectorize text data\n",
    "vectorizer = CountVectorizer(max_features=5000) # Adjust max_features as needed\n",
    "train_vectors = vectorizer.fit_transform([' '.join(review) for review in train_reviews_processed])\n",
    "test_vectors = vectorizer.transform([' '.join(review) for review in test_reviews_processed])\n",
    "val_vectors = vectorizer.transform([' '.join(review) for review in val_reviews_processed])\n",
    "\n",
    "# Pad sequences to ensure uniform length for input to CNN\n",
    "max_len = 500 # Choose an appropriate length\n",
    "X_train = pad_sequences(train_vectors.toarray(), maxlen=max_len)\n",
    "X_test = pad_sequences(test_vectors.toarray(), maxlen=max_len)\n",
    "X_val = pad_sequences(val_vectors.toarray(), maxlen=max_len)\n",
    "\n",
    "# Define CNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vectorizer.vocabulary_.size+1, output_dim=100, input_length=max_len))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(35))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Assuming y_train, y_val, and y_test are defined and contain the corresponding labels\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Conv2D.call().\n\n\u001b[1mNegative dimension size caused by subtracting 3 from 1 for '{{node sequential_1_1/conv2d_1_1/convolution}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](sequential_1_1/ExpandDims, sequential_1_1/conv2d_1_1/convolution/ReadVariableOp)' with input shapes: [?,100,1,1], [3,3,1,32].\u001b[0m\n\nArguments received by Conv2D.call():\n  • inputs=tf.Tensor(shape=(None, 100, 1, 1), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m X_test_reshaped \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(X_test, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Train the model with reshaped input data\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_reshaped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val_reshaped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Evaluate the model on test data\u001b[39;00m\n\u001b[1;32m     12\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test_reshaped, y_test)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Conv2D.call().\n\n\u001b[1mNegative dimension size caused by subtracting 3 from 1 for '{{node sequential_1_1/conv2d_1_1/convolution}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](sequential_1_1/ExpandDims, sequential_1_1/conv2d_1_1/convolution/ReadVariableOp)' with input shapes: [?,100,1,1], [3,3,1,32].\u001b[0m\n\nArguments received by Conv2D.call():\n  • inputs=tf.Tensor(shape=(None, 100, 1, 1), dtype=float32)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Reshape sentence vectors to match the input shape of the model\n",
    "X_train_reshaped = np.expand_dims(X_train, axis=-1)\n",
    "X_val_reshaped = np.expand_dims(X_val, axis=-1)\n",
    "X_test_reshaped = np.expand_dims(X_test, axis=-1)\n",
    "\n",
    "# Train the model with reshaped input data\n",
    "history = model.fit(X_train_reshaped, y_train, epochs=10, validation_data=(X_val_reshaped, y_val))\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test_reshaped, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 170ms/step - accuracy: 0.5163 - loss: 0.6905 - val_accuracy: 0.5224 - val_loss: 0.7635\n",
      "Epoch 2/10\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 161ms/step - accuracy: 0.6510 - loss: 0.6195 - val_accuracy: 0.6982 - val_loss: 0.6010\n",
      "Epoch 3/10\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 174ms/step - accuracy: 0.6983 - loss: 0.5649 - val_accuracy: 0.7231 - val_loss: 0.5637\n",
      "Epoch 4/10\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 167ms/step - accuracy: 0.7048 - loss: 0.5519 - val_accuracy: 0.6615 - val_loss: 0.6375\n",
      "Epoch 5/10\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 196ms/step - accuracy: 0.7221 - loss: 0.5295 - val_accuracy: 0.7193 - val_loss: 0.5595\n",
      "Epoch 6/10\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 182ms/step - accuracy: 0.7346 - loss: 0.5159 - val_accuracy: 0.7474 - val_loss: 0.5434\n",
      "Epoch 7/10\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 169ms/step - accuracy: 0.7419 - loss: 0.5051 - val_accuracy: 0.7361 - val_loss: 0.5516\n",
      "Epoch 8/10\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 184ms/step - accuracy: 0.7492 - loss: 0.4951 - val_accuracy: 0.7699 - val_loss: 0.5169\n",
      "Epoch 9/10\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 184ms/step - accuracy: 0.7625 - loss: 0.4757 - val_accuracy: 0.7491 - val_loss: 0.5365\n",
      "Epoch 10/10\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 196ms/step - accuracy: 0.7714 - loss: 0.4623 - val_accuracy: 0.7119 - val_loss: 0.6250\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.6854 - loss: 0.5880\n",
      "Test Accuracy: 0.6798999905586243\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "\n",
    "# Assuming train_reviews_processed, test_reviews_processed, and val_reviews_processed are defined\n",
    "# and contain lists of tokenized reviews.\n",
    "\n",
    "# Vectorize text data\n",
    "vectorizer = CountVectorizer(max_features=5000) # Adjust max_features as needed\n",
    "train_vectors = vectorizer.fit_transform([' '.join(review) for review in train_reviews_processed])\n",
    "test_vectors = vectorizer.transform([' '.join(review) for review in test_reviews_processed])\n",
    "val_vectors = vectorizer.transform([' '.join(review) for review in val_reviews_processed])\n",
    "\n",
    "# Pad sequences to ensure uniform length for input to CNN\n",
    "max_len = 500 # Choose an appropriate length\n",
    "X_train = pad_sequences(train_vectors.toarray(), maxlen=max_len)\n",
    "X_test = pad_sequences(test_vectors.toarray(), maxlen=max_len)\n",
    "X_val = pad_sequences(val_vectors.toarray(), maxlen=max_len)\n",
    "\n",
    "# Define CNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(vectorizer.vocabulary_)+1, output_dim=100))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(3)) # Adjusted pool size to 3\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Assuming y_train, y_val, and y_test are defined and contain the corresponding labels\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "# Function to build the model with hyperparameters\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(vectorizer.vocabulary_)+1, output_dim=hp.Int('embedding_dim', min_value=50, max_value=100, step=10)))\n",
    "    \n",
    "    # Add convolutional layers\n",
    "    for i in range(hp.Int('num_conv_layers', 1, 3)):\n",
    "        model.add(Conv1D(filters=hp.Int(f'conv_{i}_filters', min_value=64, max_value=128, step=16), \n",
    "                         kernel_size=hp.Int(f'conv_{i}_kernel_size', min_value=3, max_value=5, step=2), \n",
    "                         activation='relu'))\n",
    "        model.add(MaxPooling1D(pool_size=hp.Int(f'pool_{i}_size', min_value=2, max_value=5, step=1)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=hp.Int('dense_units', min_value=64, max_value=128, step=16), activation='relu'))\n",
    "    model.add(Dropout(rate=hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer=Adam(hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')), \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 10m 02s]\n",
      "val_accuracy: 0.7646222313245138\n",
      "\n",
      "Best val_accuracy So Far: 0.7705555359522501\n",
      "Total elapsed time: 00h 51m 45s\n",
      "Epoch 1/10\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 132ms/step - accuracy: 0.5995 - loss: 0.6431 - val_accuracy: 0.7555 - val_loss: 0.5300\n",
      "Epoch 2/10\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 119ms/step - accuracy: 0.7244 - loss: 0.5361 - val_accuracy: 0.7716 - val_loss: 0.5084\n",
      "Epoch 3/10\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 119ms/step - accuracy: 0.7412 - loss: 0.5084 - val_accuracy: 0.7423 - val_loss: 0.5305\n",
      "Epoch 4/10\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 111ms/step - accuracy: 0.7505 - loss: 0.4995 - val_accuracy: 0.7684 - val_loss: 0.5009\n",
      "Epoch 5/10\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 117ms/step - accuracy: 0.7664 - loss: 0.4764 - val_accuracy: 0.7523 - val_loss: 0.5235\n",
      "Epoch 6/10\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 112ms/step - accuracy: 0.7773 - loss: 0.4601 - val_accuracy: 0.7424 - val_loss: 0.5354\n",
      "Epoch 7/10\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 136ms/step - accuracy: 0.7875 - loss: 0.4436 - val_accuracy: 0.7478 - val_loss: 0.5337\n",
      "Epoch 8/10\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 129ms/step - accuracy: 0.8039 - loss: 0.4200 - val_accuracy: 0.7646 - val_loss: 0.5166\n",
      "Epoch 9/10\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 134ms/step - accuracy: 0.8122 - loss: 0.4011 - val_accuracy: 0.7355 - val_loss: 0.5675\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.6644 - loss: 0.5766\n",
      "Test Accuracy: 0.6586999893188477\n"
     ]
    }
   ],
   "source": [
    "# Create tuner\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=3,\n",
    "    directory='my_dir',\n",
    "    project_name='helloworld')\n",
    "\n",
    "# Search space summary\n",
    "tuner.search_space_summary()\n",
    "\n",
    "# Run hyperparameter search\n",
    "tuner.search(X_train, y_train, epochs=5, validation_data=(X_val, y_val))\n",
    "\n",
    "# Get best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Build model with best hyperparameters\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [00h 19m 47s]\n",
      "val_accuracy: 0.8333333134651184\n",
      "\n",
      "Best val_accuracy So Far: 0.8333333134651184\n",
      "Total elapsed time: 00h 19m 47s\n",
      "\n",
      "Search: Running Trial #2\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "100               |250               |embedding_dim\n",
      "2                 |3                 |num_conv_layers\n",
      "96                |64                |conv_0_filters\n",
      "5                 |3                 |conv_0_kernel_size\n",
      "sigmoid           |sigmoid           |conv_0_activation\n",
      "2.1388e-06        |7.8941e-06        |conv_0_kernel_regularizer\n",
      "5                 |4                 |pool_0_size\n",
      "64                |128               |conv_1_filters\n",
      "5                 |5                 |conv_1_kernel_size\n",
      "tanh              |sigmoid           |conv_1_activation\n",
      "3.4284e-05        |1.7368e-05        |conv_1_kernel_regularizer\n",
      "4                 |3                 |pool_1_size\n",
      "224               |128               |dense_units\n",
      "relu              |sigmoid           |dense_activation\n",
      "0.5               |0.3               |dropout_rate\n",
      "rmsprop           |adam              |optimizer\n",
      "0.0003563         |0.00052079        |learning_rate\n",
      "96                |64                |conv_2_filters\n",
      "5                 |3                 |conv_2_kernel_size\n",
      "tanh              |relu              |conv_2_activation\n",
      "1.6856e-05        |1e-06             |conv_2_kernel_regularizer\n",
      "3                 |2                 |pool_2_size\n",
      "\n",
      "Epoch 1/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 47ms/step - accuracy: 0.4947 - loss: 0.7290 - val_accuracy: 0.8333 - val_loss: 0.6067\n",
      "Epoch 2/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 45ms/step - accuracy: 0.5034 - loss: 0.7023 - val_accuracy: 0.8333 - val_loss: 0.6849\n",
      "Epoch 3/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 44ms/step - accuracy: 0.4992 - loss: 0.6965 - val_accuracy: 0.8333 - val_loss: 0.6950\n",
      "Epoch 4/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 43ms/step - accuracy: 0.5042 - loss: 0.6956 - val_accuracy: 0.1667 - val_loss: 0.6956\n",
      "Epoch 5/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 44ms/step - accuracy: 0.5014 - loss: 0.6953 - val_accuracy: 0.8333 - val_loss: 0.6947\n",
      "Epoch 6/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 44ms/step - accuracy: 0.5008 - loss: 0.6951 - val_accuracy: 0.8333 - val_loss: 0.6940\n",
      "Epoch 7/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 52ms/step - accuracy: 0.4983 - loss: 0.6950 - val_accuracy: 0.8333 - val_loss: 0.6939\n",
      "Epoch 8/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 47ms/step - accuracy: 0.4996 - loss: 0.6947 - val_accuracy: 0.8333 - val_loss: 0.6939\n",
      "Epoch 9/10\n",
      "\u001b[1m170/782\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 50ms/step - accuracy: 0.4978 - loss: 0.6946"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "# Function to build the model with hyperparameters\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(vectorizer.vocabulary_)+1, output_dim=hp.Int('embedding_dim', min_value=100, max_value=300, step=50)))\n",
    "    \n",
    "    # Add convolutional layers\n",
    "    for i in range(hp.Int('num_conv_layers', 2, 4)):\n",
    "        model.add(Conv1D(filters=hp.Int(f'conv_{i}_filters', min_value=64, max_value=128, step=16), \n",
    "                         kernel_size=hp.Int(f'conv_{i}_kernel_size', min_value=3, max_value=7, step=2), \n",
    "                         activation=hp.Choice(f'conv_{i}_activation', ['relu', 'tanh', 'sigmoid']),\n",
    "                         kernel_regularizer=l2(hp.Float(f'conv_{i}_kernel_regularizer', min_value=1e-6, max_value=1e-3, sampling='log'))))\n",
    "        model.add(MaxPooling1D(pool_size=hp.Int(f'pool_{i}_size', min_value=2, max_value=5, step=1)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=hp.Int('dense_units', min_value=128, max_value=256, step=32), activation=hp.Choice('dense_activation', ['relu', 'tanh', 'sigmoid'])))\n",
    "    model.add(Dropout(rate=hp.Float('dropout_rate', min_value=0.3, max_value=0.6, step=0.1)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    optimizer_choice = hp.Choice('optimizer', ['adam', 'rmsprop'])\n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-3, sampling='log')\n",
    "    if optimizer_choice == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create tuner\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,  # Increase the number of trials for better exploration\n",
    "    executions_per_trial=3,\n",
    "    directory='my2_dir',\n",
    "    project_name='helloworld_v2')\n",
    "\n",
    "# Search space summary\n",
    "tuner.search_space_summary()\n",
    "\n",
    "# Run hyperparameter search\n",
    "tuner.search(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
    "\n",
    "# Get best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Build model with best hyperparameters\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=128, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
