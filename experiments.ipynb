{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_7/q65t_5bs3653ct73dh3s10w00000gn/T/ipykernel_4316/4057663228.py:26: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 1394\n",
      "Vocabulary Size of Training Set: 64602\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gensim\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import contractions\n",
    "# Define function to load dataset\n",
    "def load_dataset(directory, label):\n",
    "    data = []\n",
    "    for filename in os.listdir(directory):\n",
    "        with open(os.path.join(directory, filename), 'r') as file:\n",
    "            review = file.read()\n",
    "            data.append((review, label))\n",
    "    return data\n",
    "\n",
    "# Define preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Expand contractions\n",
    "    text = contractions.fix(text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove non-alphabetic tokens and convert to lowercase\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if not word in stop_words]\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "# Define function to find maximum sequence length\n",
    "def find_max_sequence_length(tokenized_texts):\n",
    "    max_length = 0\n",
    "    for tokens in tokenized_texts:\n",
    "        sequence_length = len(tokens)\n",
    "        if sequence_length > max_length:\n",
    "            max_length = sequence_length\n",
    "    return max_length\n",
    "\n",
    "# Define paths to dataset directories\n",
    "directory_path = \"/Users/mrbinit/Downloads/aclImdb\"\n",
    "train_pos_dir = os.path.join(directory_path, 'train', 'pos')\n",
    "train_neg_dir = os.path.join(directory_path, 'train', 'neg')\n",
    "test_pos_dir = os.path.join(directory_path, 'test', 'pos')\n",
    "test_neg_dir = os.path.join(directory_path, 'test', 'neg')\n",
    "val_pos_dir = os.path.join(directory_path, 'val', 'pos')\n",
    "val_neg_dir = os.path.join(directory_path, 'val', 'neg')\n",
    "\n",
    "# Load dataset\n",
    "train_data = load_dataset(train_pos_dir, 1) + load_dataset(train_neg_dir, 0)\n",
    "test_data = load_dataset(test_pos_dir, 1) + load_dataset(test_neg_dir, 0)\n",
    "val_data = test_data[:15000]\n",
    "test_data = test_data[15000:25000]\n",
    "\n",
    "# Separate reviews and labels\n",
    "train_reviews, train_labels = zip(*train_data)\n",
    "test_reviews, test_labels = zip(*test_data)\n",
    "val_reviews, val_labels = zip(*val_data)\n",
    "\n",
    "# Preprocess all the data\n",
    "train_reviews_processed = [preprocess_text(review) for review in train_reviews]\n",
    "test_reviews_processed = [preprocess_text(review) for review in test_reviews]\n",
    "val_reviews_processed = [preprocess_text(review) for review in val_reviews]\n",
    "\n",
    "# Find maximum sequence length\n",
    "max_sequence_length = find_max_sequence_length(train_reviews_processed)\n",
    "print(\"Maximum sequence length:\", max_sequence_length)\n",
    "\n",
    "# Combine tokenized words from the training set into single list\n",
    "all_tokenized_words_train = [word for review in train_reviews_processed for word in review]\n",
    "\n",
    "# Count the occurrences of each unique word in the training set\n",
    "word_counts_train = Counter(all_tokenized_words_train)\n",
    "\n",
    "# Determine the vocabulary size of the training set\n",
    "vocab_size_train = len(word_counts_train)\n",
    "print(\"Vocabulary Size of Training Set:\", vocab_size_train)\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = gensim.models.Word2Vec(sentences=train_reviews_processed + test_reviews_processed + val_reviews_processed,\n",
    "                                        vector_size=100, window=5, min_count=5, workers=4)\n",
    "\n",
    "# Function to get vector representation of a sentence\n",
    "def get_sentence_vector(tokens):\n",
    "    vector = np.zeros((100,))\n",
    "    count = 0\n",
    "    for word in tokens:\n",
    "        if word in word2vec_model.wv:\n",
    "            vector += word2vec_model.wv[word]\n",
    "            count += 1\n",
    "    if count != 0:\n",
    "        vector /= count\n",
    "    return vector\n",
    "\n",
    "# Get sentence vector for all the datasets\n",
    "train_vectors = [get_sentence_vector(tokens) for tokens in train_reviews_processed]\n",
    "test_vectors = [get_sentence_vector(tokens) for tokens in test_reviews_processed]\n",
    "val_vectors = [get_sentence_vector(tokens) for tokens in val_reviews_processed]\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X_train = np.array(train_vectors)\n",
    "X_test = np.array(test_vectors)\n",
    "X_val = np.array(val_vectors)\n",
    "\n",
    "y_train = np.array(train_labels)\n",
    "y_test = np.array(test_labels)\n",
    "y_val = np.array(val_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mrbinit/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 579us/step - accuracy: 0.7643 - loss: 0.4890 - val_accuracy: 0.8387 - val_loss: 0.3693\n",
      "Epoch 2/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 571us/step - accuracy: 0.8292 - loss: 0.3840 - val_accuracy: 0.7493 - val_loss: 0.5179\n",
      "Epoch 3/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 512us/step - accuracy: 0.8330 - loss: 0.3738 - val_accuracy: 0.8577 - val_loss: 0.3401\n",
      "Epoch 4/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 471us/step - accuracy: 0.8353 - loss: 0.3713 - val_accuracy: 0.8677 - val_loss: 0.3167\n",
      "Epoch 5/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 458us/step - accuracy: 0.8435 - loss: 0.3591 - val_accuracy: 0.8222 - val_loss: 0.3997\n",
      "Epoch 6/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 462us/step - accuracy: 0.8445 - loss: 0.3559 - val_accuracy: 0.8709 - val_loss: 0.3121\n",
      "Epoch 7/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 465us/step - accuracy: 0.8470 - loss: 0.3535 - val_accuracy: 0.8521 - val_loss: 0.3437\n",
      "Epoch 8/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 462us/step - accuracy: 0.8528 - loss: 0.3397 - val_accuracy: 0.8181 - val_loss: 0.4041\n",
      "Epoch 9/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 476us/step - accuracy: 0.8538 - loss: 0.3399 - val_accuracy: 0.8373 - val_loss: 0.3700\n",
      "Epoch 10/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 627us/step - accuracy: 0.8533 - loss: 0.3423 - val_accuracy: 0.8501 - val_loss: 0.3535\n",
      "Epoch 11/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 489us/step - accuracy: 0.8567 - loss: 0.3307 - val_accuracy: 0.7866 - val_loss: 0.4563\n",
      "Epoch 12/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 476us/step - accuracy: 0.8507 - loss: 0.3399 - val_accuracy: 0.8639 - val_loss: 0.3228\n",
      "Epoch 13/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 459us/step - accuracy: 0.8596 - loss: 0.3277 - val_accuracy: 0.8743 - val_loss: 0.2964\n",
      "Epoch 14/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 484us/step - accuracy: 0.8628 - loss: 0.3238 - val_accuracy: 0.8416 - val_loss: 0.3672\n",
      "Epoch 15/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 470us/step - accuracy: 0.8659 - loss: 0.3162 - val_accuracy: 0.8301 - val_loss: 0.3841\n",
      "Epoch 16/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 458us/step - accuracy: 0.8661 - loss: 0.3141 - val_accuracy: 0.8317 - val_loss: 0.3706\n",
      "Epoch 17/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 628us/step - accuracy: 0.8640 - loss: 0.3161 - val_accuracy: 0.8337 - val_loss: 0.3795\n",
      "Epoch 18/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 460us/step - accuracy: 0.8666 - loss: 0.3127 - val_accuracy: 0.8426 - val_loss: 0.3640\n",
      "Epoch 19/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 484us/step - accuracy: 0.8679 - loss: 0.3083 - val_accuracy: 0.8137 - val_loss: 0.4130\n",
      "Epoch 20/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 458us/step - accuracy: 0.8681 - loss: 0.3039 - val_accuracy: 0.8569 - val_loss: 0.3367\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243us/step - accuracy: 0.8171 - loss: 0.4046\n",
      "Test Loss: 0.4101601839065552\n",
      "Test Accuracy: 0.8155999779701233\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "batch_size = 32\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(100,)),  # Input layer with 100 features\n",
    "    layers.Dense(32, activation='relu'),  # Hidden layer with 32 neurons\n",
    "    layers.Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=20, \n",
    "                    validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 11s]\n",
      "val_accuracy: 0.838866651058197\n",
      "\n",
      "Best val_accuracy So Far: 0.8778666853904724\n",
      "Total elapsed time: 00h 03m 21s\n",
      "\u001b[1m188/313\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 268us/step - accuracy: 0.7559 - loss: 0.5059"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mrbinit/Library/Python/3.9/lib/python/site-packages/keras/src/saving/saving_lib.py:396: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  trackable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266us/step - accuracy: 0.7542 - loss: 0.5082\n",
      "Test accuracy: 0.7512999773025513\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from kerastuner.engine.hyperparameters import HyperParameter\n",
    "\n",
    "# Define a model-building function\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # Tune the number of units in the first dense layer\n",
    "    hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "    model.add(layers.Dense(units=hp_units, activation='relu'))\n",
    "\n",
    "    # Tune the dropout rate\n",
    "    hp_dropout = hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.1)\n",
    "    model.add(layers.Dropout(rate=hp_dropout))\n",
    "\n",
    "    # Tune the kernel regularization\n",
    "    hp_reg = hp.Choice('regularization', values=['l1', 'l2'])\n",
    "    if hp_reg == 'l1':\n",
    "        reg = keras.regularizers.l1(hp.Float('lambda', 1e-5, 1e-1, sampling='log'))\n",
    "    else:\n",
    "        reg = keras.regularizers.l2(hp.Float('lambda', 1e-5, 1e-1, sampling='log'))\n",
    "\n",
    "    # Tune the kernel initializer\n",
    "    hp_init = hp.Choice('initializer', values=['glorot_uniform', 'he_normal'])\n",
    "    if hp_init == 'glorot_uniform':\n",
    "        init = 'glorot_uniform'\n",
    "    else:\n",
    "        init = 'he_normal'\n",
    "\n",
    "    # Add output layer\n",
    "    model.add(layers.Dense(1, activation='sigmoid', kernel_regularizer=reg, kernel_initializer=init))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=keras.optimizers.Adam(),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    directory='keras_tuner_results',\n",
    "    project_name='hyperparameter_tuning')\n",
    "\n",
    "# Perform hyperparameter search\n",
    "tuner.search(X_train, y_train, validation_data=(X_val, y_val), epochs=10)\n",
    "\n",
    "# Get the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Evaluate the best model\n",
    "loss, accuracy = best_model.evaluate(X_test, y_test)\n",
    "print(f'Test accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
