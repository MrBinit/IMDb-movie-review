{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mrbinit/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 25000\n",
      "Test dataset length: 10000\n",
      "Validation dataset length: 15000\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(directory, label):\n",
    "    data = []\n",
    "    for filename in os.listdir(directory):\n",
    "        with open(os.path.join(directory, filename), 'r') as file:\n",
    "            review = file.read()\n",
    "            data.append((review, label))\n",
    "    return data\n",
    "\n",
    "directory_path = \"/Users/mrbinit/Downloads/aclImdb\" \n",
    "\n",
    "train_pos_dir = os.path.join(directory_path, 'train', 'pos')\n",
    "train_neg_dir = os.path.join(directory_path, 'train', 'neg')\n",
    "test_pos_dir = os.path.join(directory_path, 'test', 'pos')\n",
    "test_neg_dir = os.path.join(directory_path, 'test', 'neg')\n",
    "val_pos_dir = os.path.join(directory_path, 'val', 'pos')\n",
    "val_neg_dir = os.path.join(directory_path, 'val', 'neg')\n",
    "\n",
    "train_data = load_dataset(train_pos_dir, 1) + load_dataset(train_neg_dir, 0) #1 represents positive and 0 represents negative sentiments\n",
    "test_data = load_dataset(test_pos_dir, 1) + load_dataset(test_neg_dir, 0)\n",
    "\n",
    "#split the test set into a validation set (15,000 samples) and a test set (10,000 samples)\n",
    "val_data = test_data[:15000]\n",
    "test_data = test_data[15000:25000]\n",
    "\n",
    "#separate the reviews and labels from the train, test, and validation data\n",
    "train_reviews, train_labels = zip(*train_data)\n",
    "test_reviews, test_labels = zip(*test_data)\n",
    "val_reviews, val_labels = zip(*val_data)\n",
    "\n",
    "#check the lengths of train, test, and validation datasets\n",
    "train_length = len(train_reviews)\n",
    "test_length = len(test_reviews)\n",
    "val_length = len(val_reviews)\n",
    "\n",
    "print(\"Train dataset length:\", train_length)\n",
    "print(\"Test dataset length:\", test_length)\n",
    "print(\"Validation dataset length:\", val_length)\n",
    "\n",
    "\n",
    "# # convert lists to tensors\n",
    "# train_reviews = tf.convert_to_tensor(train_reviews) # tensor are used to represent multi dimensional arrays which are important for GPU computation\n",
    "# train_labels = tf.convert_to_tensor(train_labels)\n",
    "# test_reviews = tf.convert_to_tensor(test_reviews)\n",
    "# test_labels = tf.convert_to_tensor(test_labels)\n",
    "# val_reviews = tf.convert_to_tensor(val_reviews)\n",
    "# val_labels = tf.convert_to_tensor(val_labels)\n",
    "\n",
    "# # Create datasets from tensor\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices((train_reviews, train_labels)).batch(32)\n",
    "# test_dataset = tf.data.Dataset.from_tensor_slices((test_reviews, test_labels)).batch(32)\n",
    "# val_dataset = tf.data.Dataset.from_tensor_slices((val_reviews, val_labels)).batch(32)\n",
    "\n",
    "# # Shuffle the training dataset\n",
    "# train_dataset = train_dataset.shuffle(len(train_data))\n",
    "# test_dataset = test_dataset.shuffle(len(test_data))\n",
    "# val_dataset = val_dataset.shuffle(len(val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#91% accuracy wiht over fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset contains HTML tags: True\n",
      "Test dataset contains HTML tags: True\n",
      "Validation dataset contains HTML tags: True\n"
     ]
    }
   ],
   "source": [
    "#Regular expressions (regex) are sequences of characters that define a search pattern. They are used for string manipulation, searching, and pattern matching within text. \n",
    "import re\n",
    "def has_html_tags(text):\n",
    "    pattern = re.compile(r'<[^>]+>')  # Regular expression to match HTML tags\n",
    "    return bool(pattern.search(text))\n",
    "\n",
    "# Check for HTML tags in each dataset\n",
    "def check_html_tags(dataset):\n",
    "    for review, _ in dataset:\n",
    "        if has_html_tags(review):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "#check for HTML tags in each dataset\n",
    "train_has_html = check_html_tags(train_data)\n",
    "test_has_html = check_html_tags(test_data)\n",
    "val_has_html = check_html_tags(val_data)\n",
    "#print output\n",
    "print(\"Train dataset contains HTML tags:\", train_has_html)\n",
    "print(\"Test dataset contains HTML tags:\", test_has_html)\n",
    "print(\"Validation dataset contains HTML tags:\", val_has_html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset contains URLs: True\n",
      "Test dataset contains URLs: True\n",
      "Validation dataset contains URLs: True\n"
     ]
    }
   ],
   "source": [
    "def has_url(text):\n",
    "    pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    return bool(pattern.search(text))\n",
    "\n",
    "#condition to check for url\n",
    "def check_for_urls(dataset):\n",
    "    for review, _ in dataset:\n",
    "        if has_url(review):\n",
    "            return True\n",
    "    return False\n",
    "#check whether there is URL or not\n",
    "train_has_url = check_for_urls(train_data)\n",
    "test_has_url = check_for_urls(test_data)\n",
    "val_has_url = check_for_urls(val_data)\n",
    "\n",
    "print(\"Train dataset contains URLs:\", train_has_url)\n",
    "print(\"Test dataset contains URLs:\", test_has_url)\n",
    "print(\"Validation dataset contains URLs:\", val_has_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset contains special characters: True\n",
      "Test dataset contains special characters: True\n",
      "Validation dataset contains special characters: True\n"
     ]
    }
   ],
   "source": [
    "def has_special_characters(text):\n",
    "    pattern = re.compile(r'[^a-zA-Z0-9\\s]')\n",
    "    return bool(pattern.search(text))\n",
    "\n",
    "def check_for_special_characters(dataset):\n",
    "    for review, _ in dataset:\n",
    "        if has_special_characters(review):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "train_has_special_chars = check_for_special_characters(train_data)\n",
    "test_has_special_chars = check_for_special_characters(test_data)\n",
    "val_has_special_chars = check_for_special_characters(val_data)\n",
    "print(\"Train dataset contains special characters:\", train_has_special_chars)\n",
    "print(\"Test dataset contains special characters:\", test_has_special_chars)\n",
    "print(\"Validation dataset contains special characters:\", val_has_special_chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mrbinit/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/mrbinit/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import contractions\n",
    "from spellchecker import SpellChecker  \n",
    "\n",
    "#load the English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#initialize the spell checker\n",
    "spell_checker = SpellChecker()\n",
    "\n",
    "#identifies stop words and removes\n",
    "def remove_Stop_words(text):\n",
    "    #expand contraction \n",
    "    expanded_text = contractions.fix(text)\n",
    "    words = word_tokenize(expanded_text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def data_cleaning(text):\n",
    "    #convert text to lower case\n",
    "    text = text.lower()\n",
    "    #remove HTML tags\n",
    "    clean_text = re.sub(r'<.*?>', '', text)\n",
    "    #remove URLs\n",
    "    clean_text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', clean_text)\n",
    "    #remove special characters\n",
    "    clean_text = re.sub(r'[^a-zA-Z0-9\\s]', '', clean_text)\n",
    "    #handles stop words\n",
    "    clean_text = remove_Stop_words(clean_text)\n",
    "    return clean_text\n",
    "\n",
    "#function to extend words using spaCy\n",
    "def extend_words_with_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    extended_text = ' '.join([token.lemma_ for token in doc])\n",
    "    return extended_text\n",
    "\n",
    "#function to check spelling\n",
    "def spell_check(text):\n",
    "    corrected_text = ' '.join([spell_checker.correction(word) for word in text.split()])\n",
    "    return corrected_text\n",
    "\n",
    "# Apply data cleaning, lemmatization. and word extension to validation data\n",
    "cleaned_train_data = []\n",
    "for review, label in train_data:\n",
    "    cleaned_review = data_cleaning(review)\n",
    "# Check if cleaned_review is not None\n",
    "    if cleaned_review is not None:  \n",
    "        cleaned_review = extend_words_with_spacy(cleaned_review)\n",
    "        cleaned_train_data.append((cleaned_review, label))\n",
    "    else:\n",
    "        print(\"Skipping review with None result after cleaning:\", review)\n",
    "\n",
    "# Apply data cleaning, lemmatization. and word extension to test data\n",
    "cleaned_test_data = [(extend_words_with_spacy(data_cleaning(review)), label) for review, label in test_data]\n",
    "\n",
    "# Apply data cleaning, lemmatization. and word extension to validation data\n",
    "cleaned_val_data = [(extend_words_with_spacy(data_cleaning(review)), label) for review, label in val_data]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Train Data: [('movie get respect sure lot memorable quote list gem imagine movie joe piscopo actually funny maureen stapleton scene stealer moroni character absolute scream watch alan skipper hale jr police sgt', 1), ('bizarre horror movie fill famous face steal cristina rain later tvs flamingo road pretty somewhat unstable model gummy smile slate pay attempt suicide guard gateway hell scene rain model well capture mood music perfect deborah raffin charming cristinas pal raine move creepy brooklyn height brownstone inhabit blind priest top floor thing really start cook neighbor include fantastically wicked burgess meredith kinky couple sylvia miles beverly dangelo diabolical lot eli wallach great fun wily police detective movie nearly crosspollination rosemarys baby exorcistbut combination base bestseller jeffrey konvitz sentinel entertainingly spooky full shock bring well director michael winner mount thoughtfully downbeat end skill 12', 1), ('solid unremarkable film matthau einstein wonderful favorite part thing would make go way see wonderful scene physicist play badmitton love sweater conversation wait robbin retrieve birdie', 1), ('strange feeling sit alone theater occupy parent rollick kid feel like instead movie ticket give nambla membershipbase upon thomas rockwell respected book eat fry worm start like children story move new town new kid fifth grader billy forrester popular start anew make friend never easy especially prospect poindexter adam erica 4 12 foot giantfurther complicate thing joe bully freckle face sleeveless shirt daunt antagonize kid death ring crackerjack ring rumor kill punch immediately death ring unleash poison kill eight gradejoe axis evil welcome billy smuggle handful slimy worms thermos discover billy play cool swearing eat worms time throw joe face ewww win billy reluctantly bet eat 10 worm fry boil marinated hot sauce squashed spread peanut butter sandwich meal dub exotic name like radioactive slime delight kid finally live dream microwave live organismif ever meet know uncontrollably hearty laugh feel like creep erupting toddler whine dilly dick hurt fry worm wonderfully disgusting like grated farrelly brother film vomitous delightfulwriterdirector bob dolman also savvy storyteller raise stake worm must consumed 7 pm addition billy hold dark secret ultrasensitive stomachdolman also keen sense perspective accuracy draw children insecurity tendency exaggerate mundane dilemmasif hyperbolize movie way kid quandary see essentially war freedomfighter freedomhater use pubescent boy pawn proxy war learn valuable lesson unity international leader learn thing two global peacekeeping fry wormsat end film comfort two chaperone mother behind look befuddlement agree great movie great register lawful database', 1), ('probably already know 5 additional episode never air view abccom watch lot television year possibly favorite show ever crime beautifully write act show cancel actor play laura whit carlos mae damian anya omg steven caseman incredible natural role even kid great wonderful show sad go course wonder reason cancel way ill let believe ms moynahans pregnancy anything perfect time slot market watch episode abccom hope come dvd day thank read', 1)]\n",
      "Cleaned Test Data: [('oh laughedthis allan asianwhite family disabled asian boyeverythe healthy person need see eye bbcwhat utter tribe total insult eye view rubbish one episode one episode onlywhen think quality bbc put year fawlty tower example come roll init disgusting disgraceit gear politicalcorrectness devoid humour whatsoeverthis straight bowel hell would expect ultra leftwe bpci mean bbc', 0), ('see movie video couple friend part teen comedy triple header alongside dorm daze go greek obviously go quality airheade entertainment grossout gag generally get come wild roomie really stand awfulfirst impression first cover norwegian release cover show breast cover bunch seemingly crazy roomie background bring picture drunken seminake teenager bunch funny stuff stuff crazy roomie cover even feature picture funny stuff assume absent wellthis movie label comedy bit strange since even remotely funny relationship drama would accurateput shortly movie young couple inherit swanky house la force get roomie make end meet roommate eventually put young couple patience relationship test wild problem lie problem movie protagonist couple analretentive neurotic manage generate zero sympathy find root antagonist roomie halfway movie add mediocre act lame dialogue boring direction get movie well leave unseensee something else', 0), ('movie go annal fiefdom one bad time stop short say bad movie ever yet see every movie ever make make lofty claim story stale act horrible good special effect couple lbs dry ice fan somebody must relate someone get movie make mr busey mail one dog well train cute make redeem quality nevershouldhavemadeit movie two hour 3 life never get back', 0), ('show bad show ever norris family write produce direct etc etc reason ever see goofy wife like many time norris fly though air plain sight land kick obviously blind villain tree build whooshthin air always solve case good whatever skill co star ever get glory norris truly apparent norris awful stuck allow anyone one scene matter content terrible act terrible script terrible series', 0), ('look like people involve movie stuff ballot box boost rating good news apparently 18 people see suppose make 19th involvement flick know anyone longtime imdb user check vote record review past seven year promise give honest unbiased opinion come 30year horror fan also appear couple lowbudget flick himselfaside couple interesting video effect frankenstein bloody nightmare incoherent boring technically flawed beyond reason apparently shoot silent stock audio dub sound like record tin piece string anyhow three quarter dialog inaudiblei watch begin end idea story even one seem like director mostly impress long pan shot corner table dead black space nothing pad film would problem one actually develop plot make film sense pace case though rule apply matter scene shoot add storywatche video exercise futility every level whatever people work write however try influence rating imdb bad tedious stuffthat honest truth think spend money time one think easy find something well find much worseand unbiased unvarnished truth', 0)]\n",
      "Cleaned Validation Data: [('base actual story john boorman show struggle american doctor whose husband son murder continually plague loss holiday burma sister seem like good idea get away passport steal rangoon could leave country sister force stay back could get i d paper american embassy fill day could fly take trip countryside tour guide try find something stone statue nothing stir stone suddenly hell break loose catch political revolt look like escape safely board train see tour guide get beat shot split second decide jump move train try rescue think continually life danger woman demonstrate spontaneous selfless charity risk life save another patricia arquette beautiful look beautiful heart unforgettable story teach suffer one promise life always keep', 1), ('gem film four production anticipate quality indeed deliver shoot great style remind errol morris film well arrange simply grip long yet horrify point excruciating know something bad happen one guess lack participation person interview compel see bit like car accident slow motion story span conceivable aspect unlike documentary try refrain show grimmer side story also deal guilt people leave behind wonder stop time take hour get melancholy grip see verywell make documentary', 1), ('really like show drama romance comedy roll one 28 married mother identify loreleis rorys experience show watching mostly repeat family channel lately uptodate go think female would like show male know man would enjoy really like hour long half hour th hour seem fly watch give chance never see show think lorelei luke favorite character show though mainly way one another could see something take long see guess say happy viewing', 1), ('good 3d experience disney themepark certainly well original 1960 acidtrip film place league well honey shrink audience far fun barely squeak muppetvision 3d movie disneymgm even beat original 3d movie experience captain eo film relive disney great musical hit aladdin little mermaid other bring smile face throughout entire show totally kidfriendly movie unlike honey effect spectacular muppetvision', 1), ('korean movie see three really stuck first excellent horror tale two sister second third fourth park chan wook movie namely oldboy sympathy lady vengeance thirst park kind remind quentin tarantino irreverence towards convention movie shock gratuitous sense like show we expect see typically situation go radically societys moral like incest libidinous bloodsucke yet devout priest he s also quite artisticallyinclined regard cinematography movie among gorgeous seenthirst say priest repress conscienceless woman fall horror drama even comedy park disarm audience many inappropriate yet humorous situation might well work yet since two movie see lack humor element would make palatable repeat viewing', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Cleaned Train Data:\", cleaned_train_data[:5])  \n",
    "print(\"Cleaned Test Data:\", cleaned_test_data[:5])   \n",
    "print(\"Cleaned Validation Data:\", cleaned_val_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the size of train data is:  25000\n",
      "the size of test data is:  10000\n",
      "the size of valid data is:  15000\n"
     ]
    }
   ],
   "source": [
    "train_data_length = len(cleaned_train_data)\n",
    "test_data_length = len(cleaned_test_data)\n",
    "val_data_length = len(cleaned_val_data)\n",
    "print('the size of train data is: ', train_data_length)\n",
    "print('the size of test data is: ', test_data_length)\n",
    "print('the size of valid data is: ', val_data_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vector for good:  [ 0.8745212   0.37628797 -0.17627041  0.62590635 -1.6180993  -0.7731874\n",
      "  2.3806481  -0.33664715 -0.13533722  0.12433172  1.5158337   1.8610576\n",
      " -0.25381562 -0.37785384 -0.647546   -0.5520787   1.2826399   2.4887643\n",
      " -0.7832859  -1.2066302  -0.2332515  -1.0791779   0.96481985  0.311093\n",
      "  1.4078064   1.4792153  -1.4893247   0.52634    -0.35689247  0.8008382\n",
      "  2.0699937   1.1532923   2.395175    0.2864734  -2.595447    0.28501895\n",
      "  1.0909232  -0.9284192  -1.1105804  -3.3791263  -0.5798013  -0.50449145\n",
      "  1.8629632  -0.5059779   1.3483895  -0.3390586  -1.1244926  -0.5355213\n",
      "  0.23418921 -2.4655447  -0.4424566   0.41691542 -1.1824436  -0.68175083\n",
      "  0.60209674 -1.9844675   0.55258214 -1.1686139  -1.1780504  -1.485868\n",
      "  0.47644836 -0.72161597  2.176024    1.2628734  -0.15417162  1.6298772\n",
      "  0.60682523 -1.3629813  -3.1740892   0.07833279 -0.5424925   0.90062964\n",
      "  0.4414608   0.7783944  -0.44727495 -1.5391922   0.99664736  0.2733889\n",
      "  0.61202794  1.7401783   0.80234104 -2.0095863  -0.95017046  0.1462503\n",
      " -0.17100044 -0.1044523   4.6652784   0.99941766 -1.0362089   0.30621612\n",
      "  0.72445387 -0.53369856 -0.50809693 -1.4422853   0.36919144  1.2430277\n",
      "  1.7151089  -0.14282395  0.9600736  -0.07061354]\n"
     ]
    }
   ],
   "source": [
    "#tokenize the cleaned data \n",
    "tokenized_train_data = [word_tokenize(review) for review, label in cleaned_train_data]\n",
    "tokenized_test_data = [word_tokenize(review) for review, label in cleaned_test_data]\n",
    "tokenized_val_data = [word_tokenize(review) for review, label in cleaned_val_data]\n",
    "\n",
    "#train word2vec model \n",
    "word2vec_model = Word2Vec(sentences= tokenized_train_data + tokenized_test_data + tokenized_val_data, vector_size = 100, window = 5, min_count= 1, workers=4)\n",
    "print(\"Word vector for good: \", word2vec_model.wv['good'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
